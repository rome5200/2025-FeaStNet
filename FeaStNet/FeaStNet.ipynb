{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0302367f-dd10-450f-b850-8336f9785488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba96a59-96f2-498f-8547-d1eca8fe115a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715dced-68bf-470c-ad0c-256fa1d6cd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8356e1ef-7876-4a3d-b8b5-a68369792fdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from skimage import measure\n",
    "from scipy.ndimage import zoom\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c2b6f3b-4dea-4220-805b-2103cde8e852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU 메모리 점진적 할당 활성화됨\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"✅ GPU 메모리 점진적 할당 활성화됨\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03e985f-2035-4503-8b59-8a4f76842084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 시드 고정 - 랜덤성을 줄여 결과 재현 가능\\ntf.random.set_seed(config['seed'])\\nnp.random.seed(config['seed'])\\nrandom.seed(config['seed'])\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [환경 설정] 학습, 데이터, 모델 관련 설정을 한 곳에서 관리\n",
    "config = {\n",
    "    # 데이터 경로\n",
    "    'train_dir': 'C:/Users/konyang/Desktop/project2025/npy 256 파일/merged',   # 훈련 데이터 경로 (.obj 및 .npy/.eseg 포함)\n",
    "    'test_dir': 'C:/Users/konyang/Desktop/project2025/npy 256 파일/merged',     # 테스트 데이터 경로\n",
    "\n",
    "    # 모델 하이퍼파라미터\n",
    "    'input_dim': 3,                      # 정점 특징 수 (기본은 x, y, z)\n",
    "    'hidden_dim': 64,                    # GNN 중간 계층 차원 수\n",
    "    'output_dim': 2,                     # 출력 클래스 수 (예: 결절/비결절)\n",
    "\n",
    "    # 학습 설정\n",
    "    'lr': 1e-4,                          # 학습률 Ehsms 5e-5\n",
    "    'batch_size': 1,                     # GNN은 sample-by-sample이 일반적\n",
    "    'epochs': 10,                        # 학습 반복 횟수\n",
    "    'seed': 42,                          # 재현성 보장용 시드\n",
    "\n",
    "    # 입력 크기 정규화 관련 (선택)\n",
    "    'target_shape': (128, 256, 256),     # .npy 정규화 기준 (3D CT 볼륨)\n",
    "\n",
    "    # 전처리 설정\n",
    "    'resize_order': 1,                   # 1: 선형, 0: 최근접 (마스크에 적합)\n",
    "    'normalize_skip_if_same': True       # 타겟 shape이면 리사이즈 생략 여부\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# 시드 고정 - 랜덤성을 줄여 결과 재현 가능\n",
    "tf.random.set_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33817f52-fc29-4a6f-bfeb-080ed737e7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "#GPU 연결 확인\n",
    "print(tf.config.list_physical_devices('GPU')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea5e332b-5953-44f7-b31c-55794994d8a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [edge_index 생성 함수] .obj 파일에서 face 정보를 기반으로 edge_index 생성\n",
    "\n",
    "def extract_edge_index_from_faces(mesh):\n",
    "    \"\"\"\n",
    "    trimesh 객체에서 면(face) 정보를 이용하여 edge_index 생성\n",
    "    반환 형식: TensorFlow Tensor of shape [2, E]\n",
    "    \"\"\"\n",
    "    faces = np.array(mesh.faces)  # 각 face는 정점 3개로 구성됨 (N, 3)\n",
    "\n",
    "    # 각 face의 세 변을 edge로 간주\n",
    "    edges = np.vstack([\n",
    "        faces[:, [0, 1]],\n",
    "        faces[:, [1, 2]],\n",
    "        faces[:, [2, 0]],\n",
    "    ])  # shape (3 * num_faces, 2)\n",
    "\n",
    "    # 중복 제거를 위해 정렬하고 unique 처리\n",
    "    edges = np.sort(edges, axis=1)          # 각 edge 내 정점 번호 정렬\n",
    "    edges = np.unique(edges, axis=0)        # 전체에서 중복 edge 제거\n",
    "\n",
    "    # (2, E) 형식으로 전치 후 Tensor로 반환\n",
    "    return tf.convert_to_tensor(edges.T, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ed3e84e-2320-4767-893c-bbdc928322ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def downsample_graph(vertices, labels, edge_index, max_nodes=5000):\n",
    "    \"\"\"\n",
    "    정점 수가 max_nodes보다 많으면 downsampling 수행\n",
    "    - vertices: (N, 3)\n",
    "    - labels: (N,)\n",
    "    - edge_index: (2, E)\n",
    "    \"\"\"\n",
    "\n",
    "    num_nodes = vertices.shape[0]\n",
    "    if num_nodes <= max_nodes:\n",
    "        return vertices, labels, edge_index  # 그대로 반환\n",
    "\n",
    "    #무작위로 정점 선택\n",
    "    selected_indices = np.random.choice(num_nodes, size=max_nodes, replace=False)\n",
    "    selected_indices.sort()  # 정렬하면 디버깅과 유지보수에 유리함\n",
    "\n",
    "    #정점 및 라벨 downsample\n",
    "    vertices_ds = vertices[selected_indices]\n",
    "    labels_ds = labels[selected_indices]\n",
    "\n",
    "    #index 매핑 테이블 생성 (원래 idx → 새 idx)\n",
    "    old_to_new = {old: new for new, old in enumerate(selected_indices)}\n",
    "\n",
    "    #edge_index 필터링: 두 정점 모두 선택된 경우만 유지\n",
    "    edge_index_np = edge_index.numpy() if hasattr(edge_index, 'numpy') else edge_index\n",
    "    src, tgt = edge_index_np[0], edge_index_np[1]\n",
    "\n",
    "    mask = np.isin(src, selected_indices) & np.isin(tgt, selected_indices)\n",
    "    src_filtered = src[mask]\n",
    "    tgt_filtered = tgt[mask]\n",
    "\n",
    "    #새 인덱스로 edge 재매핑\n",
    "    src_mapped = np.array([old_to_new[i] for i in src_filtered])\n",
    "    tgt_mapped = np.array([old_to_new[i] for i in tgt_filtered])\n",
    "    edge_index_ds = np.stack([src_mapped, tgt_mapped], axis=0)  # shape [2, E']\n",
    "\n",
    "    return vertices_ds, labels_ds, edge_index_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cde1bd6-91df-4268-becc-ed96a3eace87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 122개의 .obj 파일이 성공적으로 로딩됨\n"
     ]
    }
   ],
   "source": [
    "def load_obj_mesh(filepath):\n",
    "    try:\n",
    "        mesh = trimesh.load(filepath, process=False)\n",
    "        vertices = np.array(mesh.vertices, dtype=np.float32)\n",
    "        labels = np.arange(vertices.shape[0], dtype=np.int32)\n",
    "        edge_index = extract_edge_index_from_faces(mesh)\n",
    "        vertices, labels, edge_index = downsample_graph(vertices, labels, edge_index, max_nodes=5000)\n",
    "        return vertices, labels, edge_index\n",
    "    except Exception as e:\n",
    "        print(f\"파일 로딩 실패: {filepath}\")\n",
    "        print(f\"오류: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# [데이터셋 로딩 함수 정의] 지정된 폴더에서 .obj 파일들을 불러와 (정점, 레이블) 쌍으로 구성된 리스트를 반환\n",
    "def load_dataset(folder):\n",
    "    data = []\n",
    "    for fname in sorted(os.listdir(folder)):\n",
    "        if fname.endswith('.obj'):\n",
    "            path = os.path.join(folder, fname)\n",
    "            v, l, e = load_obj_mesh(path)\n",
    "            if v is not None:\n",
    "                data.append((v, l, e))  \n",
    "    print(f\"총 {len(data)}개의 .obj 파일이 성공적으로 로딩됨\")\n",
    "    return data\n",
    "'''\n",
    "sample_path = os.path.join(config['train_dir'], os.listdir(config['train_dir'])[0])\n",
    "v, l = load_obj_mesh(sample_path)\n",
    "print(f\"정점 수: {len(v)}\")'''\n",
    "\n",
    "# [데이터셋 로딩 실행] 훈련/테스트 데이터셋을 지정된 폴더에서 불러오기\n",
    "train_data = load_dataset(config['train_dir'])  # 훈련용 .obj 파일들을 불러와 (정점, 레이블) 쌍 리스트로 저장\n",
    "#test_data = load_dataset(config['test_dir'])    # 테스트용 .obj 파일들을 동일 방식으로 불러"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27d63432-e0a9-4eb4-8f2b-220a1eb025c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [정점 레이블 로딩 함수] .eseg 또는 .npy 파일에서 정점별 레이블 불러오기\n",
    "\n",
    "def load_vertex_labels(label_path):\n",
    "    \"\"\"\n",
    "    정점별 레이블 파일(.eseg 또는 .npy)을 로딩하여 Tensor 반환\n",
    "    \"\"\"\n",
    "    if label_path.endswith('.npy'):\n",
    "        labels = np.load(label_path)  # (N,) shape\n",
    "    elif label_path.endswith('.eseg'):\n",
    "        with open(label_path, 'r') as f:\n",
    "            labels = np.array([int(line.strip()) for line in f if line.strip().isdigit()])\n",
    "    else:\n",
    "        raise ValueError(\"지원되지 않는 레이블 파일 형식입니다: .npy 또는 .eseg만 지원\")\n",
    "\n",
    "    return tf.convert_to_tensor(labels, dtype=tf.int32)  # (N,) shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "75bcd7cf-46e4-47e6-a814-33eecacadf3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeaStConvTF(tf.Module):\n",
    "    def __init__(self, in_channels, out_channels, heads=8, t_inv=True, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "        self.t_inv = t_inv\n",
    "\n",
    "        initializer = tf.initializers.GlorotUniform()\n",
    "\n",
    "        self.weight = tf.Variable(initializer([in_channels, heads * out_channels]), trainable=True, name=\"weight\")\n",
    "        self.u = tf.Variable(initializer([in_channels, heads]), trainable=True, name=\"u\")\n",
    "        self.c = tf.Variable(tf.zeros([heads]), trainable=True, name=\"c\")\n",
    "        self.bias = tf.Variable(tf.zeros([out_channels]), trainable=True, name=\"bias\")\n",
    "\n",
    "        if not t_inv:\n",
    "            self.v = tf.Variable(initializer([in_channels, heads]), trainable=True, name=\"v\")\n",
    "\n",
    "    def __call__(self, x, edge_index):\n",
    "        row, col = edge_index[0], edge_index[1]\n",
    "        x_i = tf.gather(x, row)\n",
    "        x_j = tf.gather(x, col)\n",
    "\n",
    "        if self.t_inv:\n",
    "            q = tf.matmul(x_i - x_j, self.u) + self.c\n",
    "        else:\n",
    "            q = tf.matmul(x_i, self.u) + tf.matmul(x_j, self.v) + self.c\n",
    "\n",
    "        q = tf.nn.softmax(q, axis=1)\n",
    "        xj_proj = tf.matmul(x_j, self.weight)\n",
    "        xj_proj = tf.reshape(xj_proj, [-1, self.heads, self.out_channels])\n",
    "\n",
    "        out = xj_proj * tf.expand_dims(q, axis=-1)\n",
    "        out = tf.reshape(out, [-1, self.heads * self.out_channels])\n",
    "        out = tf.math.unsorted_segment_mean(out, row, num_segments=tf.shape(x)[0])\n",
    "        out = tf.reshape(out, [-1, self.heads, self.out_channels])\n",
    "        out = tf.reduce_sum(out, axis=1)\n",
    "\n",
    "        return out + self.bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab24c57d-f4f8-4997-b4a5-da1b8f738be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeaStNetTF(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, t_inv=True, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.conv1 = FeaStConvTF(input_dim, hidden_dim, t_inv=t_inv, name=\"conv1\")\n",
    "        self.conv2 = FeaStConvTF(hidden_dim, hidden_dim, t_inv=t_inv, name=\"conv2\")\n",
    "\n",
    "        # 정점별 검출기\n",
    "        self.seg_fc = tf.Variable(tf.random.normal([hidden_dim, 1], stddev=0.1), trainable=True, name=\"seg_fc\")\n",
    "        self.seg_bias = tf.Variable(tf.zeros([1]), trainable=True, name=\"seg_bias\")\n",
    "\n",
    "    def __call__(self, x, edge_index, training=False):\n",
    "        h = tf.nn.relu(self.conv1(x, edge_index))\n",
    "        h = tf.nn.relu(self.conv2(h, edge_index))\n",
    "\n",
    "        seg_logits = tf.matmul(h, self.seg_fc) + self.seg_bias  # shape: (N, 1)\n",
    "        return tf.squeeze(seg_logits, axis=1)  # shape: (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24dc1a2c-a261-4eb5-8379-adabe5de041a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [모델 및 최적화기 초기화] FeaStNetTF 모델 생성 및 Adam 옵티마이저 설정\n",
    "\n",
    "# Adam 옵티마이저 정의 (학습률은 config에서 설정한 값 사용)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=config['lr'])\n",
    "\n",
    "# 모델 인스턴스 생성: 입력 차원, 은닉 차원, 출력 클래스 수를 전달\n",
    "model = FeaStNetTF(\n",
    "    input_dim=config['input_dim'],\n",
    "    hidden_dim=config['hidden_dim'],\n",
    "    t_inv=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87633992-61f2-4e3c-8e00-1da6423ef89b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_variables(model):\n",
    "    variables = []\n",
    "    for attr in model.__dict__.values():\n",
    "        if isinstance(attr, tf.Variable):\n",
    "            variables.append(attr)\n",
    "        elif isinstance(attr, tf.Module):  # conv1, conv2 내부도 포함\n",
    "            variables.extend(get_model_variables(attr))\n",
    "    return variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91a41bd2-a482-426f-9c3e-289b2f2a2dd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(dataset, threshold=0.5):\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for v_np, l_np, edge_index in dataset:\n",
    "        N = len(v_np)\n",
    "        l_np_cleaned = l_np[l_np < N]\n",
    "        l_np_binary = np.zeros(N, dtype=np.float32)\n",
    "        l_np_binary[l_np_cleaned] = 1.0\n",
    "\n",
    "        x = tf.convert_to_tensor(v_np, dtype=tf.float32)\n",
    "        y = tf.convert_to_tensor(l_np_binary, dtype=tf.float32)\n",
    "\n",
    "        logits = model(x, edge_index, training=False)\n",
    "        probs = tf.sigmoid(logits)\n",
    "        preds = tf.cast(probs > threshold, tf.float32)\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y)\n",
    "\n",
    "    preds_concat = tf.concat(all_preds, axis=0)\n",
    "    labels_concat = tf.concat(all_labels, axis=0)\n",
    "\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(preds_concat, labels_concat), tf.float32)).numpy()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ff8d2058-65ab-4ef3-ad82-c99076b98a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits mean: -1.211158\n",
      "logits std: 0.739871\n",
      "loss (sample): 0.3191718\n",
      "y unique values: [0. 1.]\n",
      "Epoch 1: Loss=0.5205, Accuracy=97.86%\n",
      "logits mean: -2.5724735\n",
      "logits std: 0.6421729\n",
      "loss (sample): 0.14392419\n",
      "y unique values: [0. 1.]\n",
      "Epoch 2: Loss=0.2059, Accuracy=97.86%\n",
      "logits mean: -3.2914374\n",
      "logits std: 0.14875704\n",
      "loss (sample): 0.118090734\n",
      "y unique values: [0. 1.]\n",
      "Epoch 3: Loss=0.1179, Accuracy=97.86%\n",
      "logits mean: -3.6616876\n",
      "logits std: 0.061561324\n",
      "loss (sample): 0.11611211\n",
      "y unique values: [0. 1.]\n",
      "Epoch 4: Loss=0.1048, Accuracy=97.86%\n",
      "logits mean: -3.779268\n",
      "logits std: 0.06463014\n",
      "loss (sample): 0.11636095\n",
      "y unique values: [0. 1.]\n",
      "Epoch 5: Loss=0.1035, Accuracy=97.86%\n",
      "logits mean: -3.810223\n",
      "logits std: 0.06643585\n",
      "loss (sample): 0.11646592\n",
      "y unique values: [0. 1.]\n",
      "Epoch 6: Loss=0.1034, Accuracy=97.86%\n",
      "logits mean: -3.8160946\n",
      "logits std: 0.06517797\n",
      "loss (sample): 0.116480544\n",
      "y unique values: [0. 1.]\n",
      "Epoch 7: Loss=0.1034, Accuracy=97.86%\n",
      "logits mean: -3.8155613\n",
      "logits std: 0.056374505\n",
      "loss (sample): 0.116510436\n",
      "y unique values: [0. 1.]\n",
      "Epoch 8: Loss=0.1033, Accuracy=97.86%\n",
      "logits mean: -3.8165157\n",
      "logits std: 0.07199019\n",
      "loss (sample): 0.116505325\n",
      "y unique values: [0. 1.]\n",
      "Epoch 9: Loss=0.1033, Accuracy=97.86%\n",
      "logits mean: -3.8140302\n",
      "logits std: 0.070740245\n",
      "loss (sample): 0.11650831\n",
      "y unique values: [0. 1.]\n",
      "Epoch 10: Loss=0.1033, Accuracy=97.86%\n"
     ]
    }
   ],
   "source": [
    "# [학습 및 평가 함수 정의] 1 epoch 학습 및 정확도 평가를 수행하는 함수들\n",
    "\n",
    "def train_epoch(dataset):\n",
    "    total_loss = 0\n",
    "\n",
    "    for v_np, l_np, edge_index in dataset:\n",
    "        # 정점 수 확인\n",
    "        N = len(v_np)\n",
    "\n",
    "        # ⚠️ 라벨 이진화: l_np는 label=1인 정점들의 index (ID)\n",
    "        l_np_cleaned = l_np[l_np < N]\n",
    "        l_np_binary = np.zeros(N, dtype=np.float32)\n",
    "        l_np_binary[l_np_cleaned] = 1.0\n",
    "\n",
    "        x = tf.convert_to_tensor(v_np, dtype=tf.float32)\n",
    "        y = tf.convert_to_tensor(l_np_binary, dtype=tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, edge_index, training=True)  # (N,)\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "        variables = get_model_variables(model)\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    return total_loss / len(dataset)\n",
    "\n",
    "\n",
    "def compute_metrics(pred, target, threshold=0.5):\n",
    "    pred_bin = tf.cast(pred > threshold, tf.float32)\n",
    "    target = tf.cast(target, tf.float32)\n",
    "\n",
    "    intersection = tf.reduce_sum(pred_bin * target)\n",
    "    union = tf.reduce_sum(pred_bin) + tf.reduce_sum(target)\n",
    "    iou = intersection / (union - intersection + 1e-8)\n",
    "    dice = 2.0 * intersection / (tf.reduce_sum(pred_bin) + tf.reduce_sum(target) + 1e-8)\n",
    "    acc = tf.reduce_mean(tf.cast(tf.equal(pred_bin, target), tf.float32))\n",
    "\n",
    "    return {\n",
    "        'IoU': iou.numpy(),\n",
    "        'Dice': dice.numpy(),\n",
    "        'Accuracy': acc.numpy()\n",
    "    }\n",
    "\n",
    "# [학습 루프 실행] 에폭 단위로 학습 및 정확도 평가 반복\n",
    "for epoch in range(config['epochs']):\n",
    "    loss = train_epoch(train_data)\n",
    "    acc = evaluate(train_data)\n",
    "\n",
    "    # 디버깅: 샘플에서 확인\n",
    "    v_np, l_np, edge_index = train_data[0]\n",
    "    N = len(v_np)\n",
    "    l_np_cleaned = l_np[l_np < N]\n",
    "    l_np_binary = np.zeros(N, dtype=np.float32)\n",
    "    l_np_binary[l_np_cleaned] = 1.0\n",
    "\n",
    "    x = tf.convert_to_tensor(v_np, dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor(l_np_binary, dtype=tf.float32)\n",
    "    logits = model(x, edge_index, training=False)\n",
    "    loss_debug = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    print(\"logits mean:\", tf.reduce_mean(logits).numpy())\n",
    "    print(\"logits std:\", tf.math.reduce_std(logits).numpy())\n",
    "    print(\"loss (sample):\", loss_debug.numpy())\n",
    "    print(\"y unique values:\", tf.unique(y).y.numpy())\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Accuracy={acc:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16d86b-54cf-4c29-8125-6786a2381fc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"train_data size:\", len(train_data))\n",
    "print(\"Train dir exists:\", os.path.exists(config['train_dir']))\n",
    "print(\"Files:\", os.listdir(config['train_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843d8fc-510b-4d5c-8426-7029d80262db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files = os.listdir(train_data)\n",
    "print(f\"총 파일 수: {len(files)}\")\n",
    "print(\"예시 파일:\", files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7a695b0-0533-41ce-8b92-12d530dadf49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FeaStNetTF' object has no attribute 'get_variables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x, edge_index, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (N,)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msigmoid_cross_entropy_with_logits(labels\u001b[38;5;241m=\u001b[39my, logits\u001b[38;5;241m=\u001b[39mlogits))\n\u001b[1;32m----> 9\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_variables\u001b[49m())\n\u001b[0;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, model\u001b[38;5;241m.\u001b[39mget_variables()))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'FeaStNetTF' object has no attribute 'get_variables'"
     ]
    }
   ],
   "source": [
    "for v_np, l_np, edge_index in train_data:\n",
    "    x = tf.convert_to_tensor(v_np, dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor(l_np, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, edge_index, training=True)  # (N,)\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    grads = tape.gradient(loss, model.get_variables())\n",
    "    optimizer.apply_gradients(zip(grads, model.get_variables()))\n",
    "\n",
    "    print(\"logits shape:\", logits.shape)\n",
    "    print(\"labels shape:\", y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9a727091-ce54-46c3-862d-628fa48cd26c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l_np unique values: [ 46 108 138 215 256 372 384 452 465 466]\n",
      "l_np unique values: [ 46  77  87 155 157 164 172 175 212 218]\n",
      "l_np unique values: [ 26  40  73 164 203 290 447 452 474 487]\n"
     ]
    }
   ],
   "source": [
    "for v_np, l_np, _ in train_data[:3]:\n",
    "    print(\"l_np unique values:\", np.unique(l_np)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4fb30a91-4d7a-4a18-92a6-42c08ac80139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 1 비율: 0.0248\n"
     ]
    }
   ],
   "source": [
    "print(\"label 1 비율:\", np.mean(l_np_binary))  # 너무 적으면 불균형\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2025",
   "language": "python",
   "name": "project2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
