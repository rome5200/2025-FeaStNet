{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0302367f-dd10-450f-b850-8336f9785488",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trimesh\n",
      "  Downloading trimesh-4.6.8-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\envs\\project2025\\lib\\site-packages (from trimesh) (1.24.4)\n",
      "Downloading trimesh-4.6.8-py3-none-any.whl (709 kB)\n",
      "   ---------------------------------------- 0.0/709.3 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/709.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 709.3/709.3 kB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: trimesh\n",
      "Successfully installed trimesh-4.6.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script trimesh.exe is installed in 'C:\\ProgramData\\anaconda3\\envs\\project2025\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "pip install trimesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a03e985f-2035-4503-8b59-8a4f76842084",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntf.random.set_seed(config['seed'])\\nnp.random.seed(config['seed'])\\nrandom.seed(config['seed'])\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [환경 설정] Config 및 Seed 고정\n",
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "config = {\n",
    "    'train_dir': '경로',\n",
    "    'test_dir': '경로',\n",
    "    'input_dim': 3,          # 입력 feature 수 (예: 정점의 x, y, z 좌표 → 3차원)\n",
    "    'hidden_dim': 64,        # 은닉 계층의 노드 수 (예: Dense 또는 GNN 계층 차원)\n",
    "    'output_dim': 10,        # 출력 클래스 수 (예: 분류 클래스가 10개일 때)\n",
    "    'lr': 1e-3,              # 학습률 (learning rate)\n",
    "    'batch_size': 32,        # 배치 크기 (한 번에 학습에 사용할 샘플 수)\n",
    "    'epochs': 10,            # 전체 학습 반복 횟수\n",
    "    'seed': 42               # 랜덤 시드 (재현성 보장용)\n",
    "}\n",
    "\n",
    "\n",
    "'''\n",
    "# 시드 고정 - 랜덤성을 줄여 결과 재현 가능\n",
    "tf.random.set_seed(config['seed'])\n",
    "np.random.seed(config['seed'])\n",
    "random.seed(config['seed'])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5477496-0457-4ae0-bc44-eeed062a773f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [데이터 로딩 함수 정의] .obj 파일을 불러와 정점 좌표와 레이블을 추출하는 함수\n",
    "def load_obj_mesh(filepath):\n",
    "    mesh = trimesh.load(filepath, process=True)  # .obj 파일을 trimesh로 로드 (정점, 면 등 포함)\n",
    "    vertices = np.array(mesh.vertices, dtype=np.float32)  # 정점 좌표만 추출하여 float32 형식으로 변환 (N, 3)\n",
    "    labels = np.arange(vertices.shape[0], dtype=np.int32)  # 각 정점에 고유 인덱스를 부여하여 레이블로 사용 (예시용)\n",
    "    return vertices, labels  # 정점 좌표와 정점별 레이블을 반환\n",
    "\n",
    "# [데이터셋 로딩 함수 정의] 지정된 폴더에서 .obj 파일들을 불러와 (정점, 레이블) 쌍으로 구성된 리스트를 반환\n",
    "def load_dataset(folder):\n",
    "    data = []  # 전체 데이터셋을 저장할 리스트 초기화\n",
    "    for fname in sorted(os.listdir(folder)):  # 폴더 내 .obj 파일명을 정렬하여 반복\n",
    "        if fname.endswith('.obj'):  # .obj 파일인 경우에만 처리\n",
    "            path = os.path.join(folder, fname)  # 파일 경로 생성\n",
    "            v, l = load_obj_mesh(path)          # 정점 좌표와 정점별 레이블 불러오기\n",
    "            data.append((v, l))                 # (정점, 레이블) 튜플을 리스트에 추가\n",
    "    return data  # 전체 데이터셋 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caa0018a-4c99-457b-acc4-485e961973b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'train_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_data \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m test_data \u001b[38;5;241m=\u001b[39m load_dataset(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_dir\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'train_dir'"
     ]
    }
   ],
   "source": [
    "# [데이터셋 로딩 실행] 훈련/테스트 데이터셋을 지정된 폴더에서 불러오기\n",
    "train_data = load_dataset(config['train_dir'])  # 훈련용 .obj 파일들을 불러와 (정점, 레이블) 쌍 리스트로 저장\n",
    "test_data = load_dataset(config['test_dir'])    # 테스트용 .obj 파일들을 동일 방식으로 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde1bd6-91df-4268-becc-ed96a3eace87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [FeaStConvTF 클래스 정의] FeaStNet의 메시지 전달 계층을 TensorFlow로 구현\n",
    "\n",
    "class FeaStConvTF:\n",
    "    def __init__(self, in_channels, out_channels, heads=8, t_inv=True):\n",
    "        self.in_channels = in_channels        # 입력 차원 수\n",
    "        self.out_channels = out_channels      # 출력 차원 수\n",
    "        self.heads = heads                    # attention head 개수\n",
    "        self.t_inv = t_inv                    # translation invariant 여부\n",
    "\n",
    "        initializer = tf.initializers.GlorotUniform()  # Xavier 초기화 방식 사용\n",
    "\n",
    "        # 입력 특징을 (heads × out_channels)로 투영하는 가중치\n",
    "        self.weight = tf.Variable(initializer([in_channels, heads * out_channels]), trainable=True)\n",
    "\n",
    "        # attention score 계산을 위한 u 벡터 (translation-invariant 버전용)\n",
    "        self.u = tf.Variable(initializer([in_channels, heads]), trainable=True)\n",
    "\n",
    "        # attention score에 더할 bias 벡터\n",
    "        self.c = tf.Variable(tf.zeros([heads]), trainable=True)\n",
    "\n",
    "        # translation-invariant가 아닐 경우 추가 파라미터 v 사용\n",
    "        if not t_inv:\n",
    "            self.v = tf.Variable(initializer([in_channels, heads]), trainable=True)\n",
    "\n",
    "        # 출력 결과에 더할 bias\n",
    "        self.bias = tf.Variable(tf.zeros([out_channels]), trainable=True)\n",
    "\n",
    "    def __call__(self, x, edge_index):\n",
    "        # edge_index: [2, E] (source, target)\n",
    "        row, col = edge_index[0], edge_index[1]  # row: target idx, col: source idx\n",
    "\n",
    "        # 메시지를 받는 쪽 정점 (i)와 주는 쪽 정점 (j)의 특징 추출\n",
    "        x_i = tf.gather(x, row)  # [E, F]\n",
    "        x_j = tf.gather(x, col)  # [E, F]\n",
    "\n",
    "        # attention score 계산\n",
    "        if self.t_inv:\n",
    "            q = tf.matmul(x_i - x_j, self.u) + self.c  # translation-invariant\n",
    "        else:\n",
    "            q = tf.matmul(x_i, self.u) + tf.matmul(x_j, self.v) + self.c  # 일반 버전\n",
    "\n",
    "        q = tf.nn.softmax(q, axis=1)  # attention 분포로 정규화: [E, heads]\n",
    "\n",
    "        # 메시지 특징을 head × out_dim 차원으로 투영\n",
    "        xj_proj = tf.matmul(x_j, self.weight)  # [E, heads * out_channels]\n",
    "        xj_proj = tf.reshape(xj_proj, [-1, self.heads, self.out_channels])  # [E, heads, out_channels]\n",
    "\n",
    "        # attention 가중치를 곱하고, 메시지들을 합산\n",
    "        out = xj_proj * tf.expand_dims(q, axis=-1)  # [E, heads, out_channels]\n",
    "        out = tf.math.segment_mean(tf.reshape(out, [-1, self.heads * self.out_channels]), row)  # [N, heads * out_channels]\n",
    "\n",
    "        # head 차원을 다시 나눠 평균 대신 합산 (PyTorch 구조와 유사)\n",
    "        out = tf.reshape(out, [-1, self.heads, self.out_channels])  # [N, heads, out_channels]\n",
    "        out = tf.reduce_sum(out, axis=1)  # [N, out_channels]\n",
    "\n",
    "        # bias 더하고 반환\n",
    "        return out + self.bias  # [N, out_channels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab24c57d-f4f8-4997-b4a5-da1b8f738be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [FeaStNetTF 모델 정의] FeaStConv 계층을 활용한 GNN 기반 메시 분류 모델\n",
    "\n",
    "class FeaStNetTF:\n",
    "    def __init__(self, input_dim, num_classes, heads=8, t_inv=True):\n",
    "        # 첫 번째 FC 계층 (입력 특징 투영)\n",
    "        self.fc0_w = tf.Variable(tf.random.normal([input_dim, 16], stddev=0.1))  # Linear: input_dim → 16\n",
    "        self.fc0_b = tf.Variable(tf.zeros([16]))  # bias\n",
    "\n",
    "        # FeaStConv 계층 3개 (깊이 있는 메시지 전달)\n",
    "        self.conv1 = FeaStConvTF(16, 32, heads=heads, t_inv=t_inv)   # 16 → 32\n",
    "        self.conv2 = FeaStConvTF(32, 64, heads=heads, t_inv=t_inv)   # 32 → 64\n",
    "        self.conv3 = FeaStConvTF(64, 128, heads=heads, t_inv=t_inv)  # 64 → 128\n",
    "\n",
    "        # 마지막 FC 계층들 (분류기)\n",
    "        self.fc1_w = tf.Variable(tf.random.normal([128, 256], stddev=0.1))  # 128 → 256\n",
    "        self.fc1_b = tf.Variable(tf.zeros([256]))\n",
    "        self.fc2_w = tf.Variable(tf.random.normal([256, num_classes], stddev=0.1))  # 256 → num_classes\n",
    "        self.fc2_b = tf.Variable(tf.zeros([num_classes]))\n",
    "\n",
    "    def __call__(self, x, edge_index, training=False):\n",
    "        # 입력 특징 → 첫 FC 계층\n",
    "        x = tf.nn.elu(tf.matmul(x, self.fc0_w) + self.fc0_b)\n",
    "\n",
    "        # FeaStConv 계층 3단계\n",
    "        x = tf.nn.elu(self.conv1(x, edge_index))\n",
    "        x = tf.nn.elu(self.conv2(x, edge_index))\n",
    "        x = tf.nn.elu(self.conv3(x, edge_index))\n",
    "\n",
    "        # FC 계층 → 분류기\n",
    "        x = tf.nn.elu(tf.matmul(x, self.fc1_w) + self.fc1_b)\n",
    "\n",
    "        # 학습 시 드롭아웃 적용\n",
    "        if training:\n",
    "            x = tf.nn.dropout(x, rate=0.5)\n",
    "\n",
    "        # 최종 출력층\n",
    "        x = tf.matmul(x, self.fc2_w) + self.fc2_b\n",
    "\n",
    "        # 클래스별 확률 분포로 변환 (log softmax)\n",
    "        return tf.nn.log_softmax(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24dc1a2c-a261-4eb5-8379-adabe5de041a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [모델 및 최적화기 초기화] FeaStNetTF 모델 생성 및 Adam 옵티마이저 설정\n",
    "\n",
    "# Adam 옵티마이저 정의 (학습률은 config에서 설정한 값 사용)\n",
    "optimizer = tf.optimizers.Adam(learning_rate=config['lr'])\n",
    "\n",
    "# 모델 인스턴스 생성: 입력 차원, 은닉 차원, 출력 클래스 수를 전달\n",
    "model = FeaStNetTF(\n",
    "    input_dim=config['input_dim'],      # 예: 3 (정점의 x, y, z 좌표)\n",
    "    num_classes=config['output_dim'],   # 예: 2 또는 10 (클래스 수)\n",
    "    heads=8,                            # attention head 수는 기본값 사용\n",
    "    t_inv=True                          # translation invariant 설정\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff8d2058-65ab-4ef3-ad82-c99076b98a78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# [학습 및 평가 함수 정의] 1 epoch 학습 및 정확도 평가를 수행하는 함수들\n",
    "\n",
    "def train_epoch(dataset):\n",
    "    total_loss = 0  # 누적 손실 초기화\n",
    "\n",
    "    for x_np, y_np in dataset:\n",
    "        x = tf.convert_to_tensor(x_np)  # 입력 정점 특징을 텐서로 변환\n",
    "        y = tf.convert_to_tensor(y_np)  # 정점별 레이블도 텐서로 변환\n",
    "\n",
    "        # 현재는 정점 간 fully-connected edge 구성 (향후 face 기반 edge로 교체 필요)\n",
    "        num_nodes = x.shape[0]\n",
    "        row, col = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))  # 정점 쌍 생성\n",
    "        edge_index = np.vstack([row.flatten(), col.flatten()])              # shape: [2, E]\n",
    "        edge_index = tf.convert_to_tensor(edge_index, dtype=tf.int32)       # 텐서로 변환\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x, edge_index, training=True)  # 모델 forward\n",
    "            y_onehot = tf.one_hot(y, config['output_dim'])  # 레이블을 원-핫 인코딩\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y_onehot, logits))  # 손실 계산\n",
    "\n",
    "        # 모델에서 사용하는 모든 학습 가능한 변수에 대해 그래디언트 계산\n",
    "        grads = tape.gradient(loss, model.get_variables())\n",
    "        \n",
    "        # 옵티마이저를 통해 가중치 갱신\n",
    "        optimizer.apply_gradients(zip(grads, model.get_variables()))\n",
    "\n",
    "        total_loss += loss.numpy()  # 손실 누적\n",
    "\n",
    "    return total_loss / len(dataset)  # 평균 손실 반환\n",
    "\n",
    "\n",
    "def evaluate(dataset):\n",
    "    correct = 0  # 정확히 예측한 정점 수\n",
    "    total = 0    # 전체 정점 수\n",
    "\n",
    "    for x_np, y_np in dataset:\n",
    "        x = tf.convert_to_tensor(x_np)\n",
    "        y = tf.convert_to_tensor(y_np)\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        row, col = np.meshgrid(np.arange(num_nodes), np.arange(num_nodes))  # fully-connected edge\n",
    "        edge_index = np.vstack([row.flatten(), col.flatten()])\n",
    "        edge_index = tf.convert_to_tensor(edge_index, dtype=tf.int32)\n",
    "\n",
    "        logits = model(x, edge_index)  # 모델 예측\n",
    "        preds = tf.argmax(logits, axis=1, output_type=tf.int32)  # 예측 클래스\n",
    "\n",
    "        correct += tf.reduce_sum(tf.cast(tf.equal(preds, y), tf.int32)).numpy()  # 맞춘 정점 수 누적\n",
    "        total += len(y_np)  # 전체 정점 수 누적\n",
    "\n",
    "    return correct / total  # 정확도 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db678f4-f908-4bf3-8881-52b09633fba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [학습 루프 실행] 에폭 단위로 학습 및 정확도 평가 반복\n",
    "\n",
    "for epoch in range(config['epochs']):  # 지정된 에폭 수만큼 반복\n",
    "    loss = train_epoch(train_data)     # 한 에폭 동안 학습하고 평균 손실 반환\n",
    "    acc = evaluate(test_data)          # 테스트 데이터셋에 대한 정확도 평가\n",
    "    print(f\"Epoch {epoch+1}: Loss={loss:.4f}, Accuracy={acc:.2%}\")  # 결과 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2025",
   "language": "python",
   "name": "project2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
